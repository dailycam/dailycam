"""Gemini AI Tools for content curation"""

import os
import requests
from typing import List, Dict, Any, Optional
from googleapiclient.discovery import build
from tavily import TavilyClient
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup


import re

class YouTubeSearchTool:
    """YouTube ì˜ìƒ ê²€ìƒ‰ ë„êµ¬"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("YOUTUBE_API_KEY")
        if self.api_key:
            self.youtube = build('youtube', 'v3', developerKey=self.api_key)
        else:
            self.youtube = None
            print("âš ï¸ [YouTubeSearchTool] YOUTUBE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. DuckDuckGo ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def search_videos(
        self, 
        query: str, 
        max_results: int = 10,
        min_views: int = 10000
    ) -> List[Dict[str, Any]]:
        """
        YouTube ì˜ìƒ ê²€ìƒ‰
        """
        if self.youtube:
            return self._search_with_api(query, max_results, min_views)
        else:
            return self._search_with_ddg(query, max_results)
            
    def _search_with_api(self, query: str, max_results: int, min_views: int) -> List[Dict[str, Any]]:
        try:
            # ì˜ìƒ ê²€ìƒ‰
            search_response = self.youtube.search().list(
                part="snippet",
                q=query,
                type="video",
                maxResults=max_results,  # 2ë°°ìˆ˜ ì œê±°í•˜ì—¬ Quota ì ˆì•½
                relevanceLanguage="ko",
                order="relevance",
                regionCode="KR"
            ).execute()
            
            videos = []
            for item in search_response.get('items', []):
                video_id = item['id']['videoId']
                snippet = item['snippet']
                
                # ì˜ìƒ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì¡°íšŒìˆ˜ í™•ì¸)
                # Quota ì ˆì•½ì„ ìœ„í•´ ìƒì„¸ ì •ë³´ ì¡°íšŒëŠ” ìµœì†Œí™”í•˜ê±°ë‚˜ ìƒëµ ê³ ë ¤ ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” ìœ ì§€í•˜ë˜ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
                try:
                    video_response = self.youtube.videos().list(
                        part="statistics,contentDetails",
                        id=video_id
                    ).execute()
                    
                    if video_response['items']:
                        stats = video_response['items'][0]['statistics']
                        view_count = int(stats.get('viewCount', 0))
                        
                        if view_count >= min_views:
                            videos.append({
                                'video_id': video_id,
                                'title': snippet['title'],
                                'description': snippet['description'],
                                'channel': snippet['channelTitle'],
                                'thumbnail': snippet['thumbnails']['high']['url'],
                                'published_at': snippet['publishedAt'],
                                'view_count': view_count,
                                'url': f"https://www.youtube.com/watch?v={video_id}"
                            })
                except Exception:
                    # ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œìœ¼ë¡œ ì¶”ê°€ (Quota ì ˆì•½)
                    videos.append({
                        'video_id': video_id,
                        'title': snippet['title'],
                        'description': snippet['description'],
                        'channel': snippet['channelTitle'],
                        'thumbnail': snippet['thumbnails']['high']['url'],
                        'published_at': snippet['publishedAt'],
                        'view_count': 0,
                        'url': f"https://www.youtube.com/watch?v={video_id}"
                    })
                
                if len(videos) >= max_results:
                    break
            
            return videos[:max_results]
            
        except Exception as e:
            print(f"YouTube API ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            # API ì˜¤ë¥˜ ì‹œ DuckDuckGoë¡œ Fallback
            print("âš ï¸ [YouTube] API ì˜¤ë¥˜ ë°œìƒ. DuckDuckGo ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return self._search_with_ddg(query, max_results)

    def _search_with_ddg(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """DuckDuckGoë¥¼ ì‚¬ìš©í•œ YouTube ê²€ìƒ‰ (API í‚¤ ì—†ì„ ë•Œ)"""
        try:
            with DDGS() as ddgs:
                results = ddgs.videos(
                    keywords=f"{query} site:youtube.com",
                    region="kr-kr",
                    safesearch="moderate",
                    max_results=max_results * 2
                )
                
                videos = []
                for r in results:
                    # DDG ê²°ê³¼ ë§¤í•‘
                    # r = {'content': '...', 'description': '...', 'duration': '...', 'embed_html': '...', 'embed_url': '...', 'images': {...}, 'provider': 'YouTube', 'published': '...', 'publisher': '...', 'statistics': {'viewCount': ...}, 'title': '...', 'uploader': '...', 'url': '...'}
                    
                    # ì¡°íšŒìˆ˜ í™•ì¸ (DDG ê²°ê³¼ì— statisticsê°€ ìˆëŠ” ê²½ìš°)
                    view_count = 0
                    if 'statistics' in r and 'viewCount' in r['statistics']:
                        view_count = r['statistics']['viewCount'] or 0
                    elif 'views' in r: # ì¼ë¶€ ë²„ì „ì—ì„œëŠ” viewsë¡œ ì˜´
                        view_count = r['views'] or 0
                        
                    videos.append({
                        'video_id': r.get('id', ''), # DDGëŠ” IDë¥¼ ì§ì ‘ ì£¼ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
                        'title': r.get('title', ''),
                        'description': r.get('description', ''),
                        'channel': r.get('uploader', ''),
                        'thumbnail': r.get('images', {}).get('large', '') or r.get('image', ''),
                        'published_at': r.get('published', ''),
                        'view_count': view_count,
                        'url': r.get('content', '') or r.get('url', '')
                    })
                    
                    if len(videos) >= max_results:
                        break
                        
                return videos
        except Exception as e:
            print(f"DuckDuckGo ì˜ìƒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []


class WebSearchTool:
    """ì›¹ ê²€ìƒ‰ ë„êµ¬ (Tavily or DuckDuckGo)"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("TAVILY_API_KEY")
        if self.api_key:
            self.client = TavilyClient(api_key=self.api_key)
        else:
            self.client = None
            print("âš ï¸ [WebSearchTool] TAVILY_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. DuckDuckGo ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def search_blogs(
        self, 
        query: str, 
        max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ìœ¡ì•„ ë¸”ë¡œê·¸ ê²€ìƒ‰
        """
        if self.client:
            return self._search_with_tavily(query, max_results)
        else:
            return self._search_with_ddg(query, max_results)

    def _is_safe_content(self, title: str, description: str) -> bool:
        """ì½˜í…ì¸  ì•ˆì „ì„± ë° ê´€ë ¨ì„± ê²€ì‚¬"""
        # ê¸ˆì§€ì–´ ëª©ë¡ (ìŠ¤íŒ¸, ë„ë°•, ì„±ì¸, ê³¼ë„í•œ ê´‘ê³ )
        blacklist = [
            'ë„ë°•', 'ì¹´ì§€ë…¸', 'ë°”ì¹´ë¼', 'í† í† ', 'ì„±ì¸', '19ê¸ˆ', 'ì•¼ë™',
            'ëŒ€ì¶œ', 'ê¸ˆë¦¬', 'ìˆ˜ìµ', 'íˆ¬ì', 'ì£¼ì‹', 'ì½”ì¸', 'ë¶„ì–‘', 'ë§¤ë§¤',
            'ê°€ì…ì½”ë“œ', 'ì¶”ì²œì¸', 'ì´ë²¤íŠ¸', 'ë‹¹ì²¨', 'ë¬´ë£Œ', 'ì¿ í°'
        ]
        
        text = (title + " " + description).lower()
        
        # 0. í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸ (ì¤‘êµ­ì–´/ì˜ì–´ ìŠ¤íŒ¸ í•„í„°ë§)
        # í•œê¸€ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ì™¸êµ­ ì‚¬ì´íŠ¸(ì¤‘êµ­ì–´ ë“±)ë¡œ ê°„ì£¼í•˜ê³  í•„í„°ë§
        if not re.search('[ê°€-í£]', text):
            # print(f"ğŸš« [Filter] í•œê¸€ ì—†ìŒ: {title[:30]}...")
            return False
        
        # 1. ê¸ˆì§€ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸
        for word in blacklist:
            if word in text:
                return False
                
        # 2. ìœ¡ì•„ ê´€ë ¨ì„± í™•ì¸ (ì„ íƒì  - ë„ˆë¬´ ì—„ê²©í•˜ë©´ ê²°ê³¼ê°€ ì—†ì„ ìˆ˜ ìˆìŒ)
        # keywords = ['ìœ¡ì•„', 'ì•„ê¸°', 'ì•„ì´', 'ë² ì´ë¹„', 'ë§˜', 'ë¶€ëª¨', 'êµìœ¡', 'ë°œë‹¬', 'ë†€ì´']
        # if not any(k in text for k in keywords):
        #     return False
            
        return True

    def _search_with_tavily(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        try:
            # ê²€ìƒ‰ì–´ì— ê´‘ê³  ì œì™¸ í‚¤ì›Œë“œ ì¶”ê°€
            safe_query = f"{query} ìœ¡ì•„ íŒ -ê´‘ê³  -í˜‘ì°¬ -íŒë§¤"
            
            response = self.client.search(
                query=safe_query,
                search_depth="advanced",
                max_results=max_results * 4  # í•„í„°ë§ì„ ê³ ë ¤í•´ ë” ë§ì´ ìš”ì²­ (4ë°°ìˆ˜)
            )
            
            blogs = []
            for result in response.get('results', []):
                title = result.get('title', '')
                description = result.get('content', '')[:200]
                url = result.get('url', '')
                
                # ì•ˆì „ì„± ê²€ì‚¬
                if not self._is_safe_content(title, description):
                    continue
                    
                blogs.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'score': result.get('score', 0.0)
                })
                
                if len(blogs) >= max_results:
                    break
            
            return blogs
            
        except Exception as e:
            print(f"Tavily ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _search_with_ddg(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """DuckDuckGoë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê²€ìƒ‰ (API í‚¤ ì—†ì„ ë•Œ)"""
        try:
            with DDGS() as ddgs:
                # ë¸”ë¡œê·¸ í‚¤ì›Œë“œ ì¶”ê°€ ë° ê´‘ê³  ì œì™¸ (ì¤‘êµ­ ì‚¬ì´íŠ¸ ì œì™¸ ì¶”ê°€)
                search_query = f"{query} ìœ¡ì•„ ë¸”ë¡œê·¸ -ê´‘ê³  -í˜‘ì°¬ -ì¿ íŒ¡íŒŒíŠ¸ë„ˆìŠ¤ -site:.cn -ì¤‘êµ­"
                
                results = ddgs.text(
                    keywords=search_query,
                    region="kr-kr",
                    safesearch="moderate",
                    max_results=max_results * 5  # í•„í„°ë§ ê³ ë ¤í•˜ì—¬ 5ë°°ìˆ˜ ìš”ì²­
                )
                
                blogs = []
                import json
                
                for r in results:
                    # description ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                    desc = r.get('body', '')
                    if not isinstance(desc, str):
                        desc = str(desc) if desc else ''
                    
                    # DuckDuckGo JSON ì•„í‹°íŒ©íŠ¸ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    if desc.strip().startswith('{"title":'):
                        try:
                            parsed = json.loads(desc)
                            desc = parsed.get('snippet', parsed.get('body', ''))
                        except:
                            match = re.search(r'"snippet":"(.*?)(?:"[,}]|$)', desc)
                            if match:
                                desc = match.group(1).replace('\\"', '"').replace('\\n', ' ')
                            else:
                                desc = ''
                    
                    # HTML íƒœê·¸ ë° ë‚¨ì€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
                    desc = re.sub(r'<[^>]+>', '', desc)
                    desc = desc.replace('{"title":', '').replace('"source":', '')
                    
                    title = r.get('title', '')
                    
                    # URL ì¶”ì¶œ (ì—¬ëŸ¬ í‚¤ ì‹œë„)
                    url = r.get('href') or r.get('link') or r.get('url', '')
                    
                    # 0. ë„ë©”ì¸ í•„í„°ë§ (ì¤‘êµ­ ì‚¬ì´íŠ¸ ê°•ë ¥ ì°¨ë‹¨)
                    if any(domain in url for domain in ['.cn', 'zhihu.com', 'baidu.com', '163.com', 'qq.com', 'bilibili.com']):
                        print(f"ğŸš« [Filter] ì¤‘êµ­ ë„ë©”ì¸ ì°¨ë‹¨: {url}")
                        continue

                    # ì•ˆì „ì„± ê²€ì‚¬ (í•œê¸€ í¬í•¨ ì—¬ë¶€ ë“±)
                    if not self._is_safe_content(title, desc):
                        continue
                    
                    # ë””ë²„ê·¸ ë¡œê¹…
                    print(f"ğŸ“ [Blog] ì œëª©: {title[:50]}")
                    print(f"ğŸ”— [Blog] URL: {url}")
                    
                    blogs.append({
                        'title': title,
                        'description': desc.strip()[:200],
                        'url': url,
                        'score': 0.0
                    })
                    
                    if len(blogs) >= max_results:
                        break
                    
                return blogs
        except Exception as e:
            print(f"DuckDuckGo ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def search_news(
        self, 
        query: str, 
        max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ë‰´ìŠ¤ ê²€ìƒ‰ (DuckDuckGo news ë©”ì„œë“œ ì‚¬ìš©)
        """
        try:
            print(f"ğŸ” [News] ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            with DDGS() as ddgs:
                # DuckDuckGoì˜ news() ë©”ì„œë“œ ì‚¬ìš©
                results = ddgs.news(
                    keywords=query,
                    region="kr-kr",
                    safesearch="moderate",
                    max_results=max_results
                )
                
                news = []
                result_count = 0
                
                for r in results:
                    result_count += 1
                    print(f"ğŸ“° [News] ê²°ê³¼ {result_count}: {r.get('title', 'No title')[:50]}")
                    
                    # ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹œë„
                    thumbnail = r.get('image', None)
                    
                    # DuckDuckGoì—ì„œ ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not thumbnail:
                        url = r.get('url', '')
                        if url:
                            print(f"ğŸ” [News] URLì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ ì‹œë„: {url[:50]}...")
                            thumbnail = extract_blog_thumbnail(url)
                    
                    if thumbnail:
                        print(f"âœ… [News] ì¸ë„¤ì¼: {thumbnail[:80]}...")
                    else:
                        print(f"âš ï¸ [News] ì¸ë„¤ì¼ ì—†ìŒ")
                    
                    news.append({
                        'title': r.get('title', ''),
                        'description': r.get('body', '')[:200],
                        'url': r.get('url', ''),
                        'thumbnail': thumbnail,
                        'score': 0.0
                    })
                
                print(f"âœ… [News] ì´ {len(news)}ê°œ ê²°ê³¼ ë°˜í™˜")
                return news
        except Exception as e:
            print(f"âŒ [News] DuckDuckGo ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return []


def extract_blog_thumbnail(url: str) -> Optional[str]:
    """ë¸”ë¡œê·¸ URLì—ì„œ Open Graph ì¸ë„¤ì¼ ì¶”ì¶œ"""
    try:
        print(f"ğŸ–¼ï¸ [Thumbnail] ì¶”ì¶œ ì‹œë„: {url}")
        
        # User-Agent ì„¤ì • (ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì ‘ê·¼ ì‹œ í•„ìš”)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Open Graph ì´ë¯¸ì§€ ì°¾ê¸°
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            thumbnail_url = og_image['content']
            print(f"âœ… [Thumbnail] OG ì´ë¯¸ì§€ ë°œê²¬: {thumbnail_url[:80]}...")
            return thumbnail_url
        
        # Twitter ì¹´ë“œ ì´ë¯¸ì§€ ì°¾ê¸° (ëŒ€ì²´)
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            thumbnail_url = twitter_image['content']
            print(f"âœ… [Thumbnail] Twitter ì´ë¯¸ì§€ ë°œê²¬: {thumbnail_url[:80]}...")
            return thumbnail_url
        
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì°¾ê¸° (ìµœí›„ì˜ ìˆ˜ë‹¨)
        first_img = soup.find('img')
        if first_img and first_img.get('src'):
            img_src = first_img['src']
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_src.startswith('//'):
                thumbnail_url = 'https:' + img_src
                print(f"âœ… [Thumbnail] ì²« ì´ë¯¸ì§€ ë°œê²¬ (//): {thumbnail_url[:80]}...")
                return thumbnail_url
            elif img_src.startswith('/'):
                from urllib.parse import urlparse
                parsed = urlparse(url)
                thumbnail_url = f"{parsed.scheme}://{parsed.netloc}{img_src}"
                print(f"âœ… [Thumbnail] ì²« ì´ë¯¸ì§€ ë°œê²¬ (/): {thumbnail_url[:80]}...")
                return thumbnail_url
            print(f"âœ… [Thumbnail] ì²« ì´ë¯¸ì§€ ë°œê²¬: {img_src[:80]}...")
            return img_src
        
        print(f"âš ï¸ [Thumbnail] ì´ë¯¸ì§€ ì—†ìŒ: {url}")
        return None
    except Exception as e:
        print(f"âŒ [Thumbnail] ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
        return None

